{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtKDXALuYWKi",
        "outputId": "cdddce80-5576-4194-f30e-8b947e0c0db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vmS6SAJ-YWKl"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vto6bptGYWKm",
        "outputId": "8c630665-ad17-48e4-a3a3-c6bd17888afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance can be seen in several aspects:\n",
            "\n",
            "1. **Real-time Applications**: Fast language models enable real-time applications such as chatbots, virtual assistants, and language translation systems to respond quickly and efficiently. This is particularly important in customer-facing applications where delayed responses can lead to frustration and a negative user experience.\n",
            "2. **Low-Latency Requirements**: Many applications, such as voice assistants, require language models to process and respond to user input within a few hundred milliseconds. Fast language models can meet these low-latency requirements, ensuring a seamless user experience.\n",
            "3. **Scalability**: Fast language models can handle large volumes of data and user requests, making them essential for large-scale NLP applications such as language translation, sentiment analysis, and text summarization.\n",
            "4. **Energy Efficiency**: Fast language models can reduce the computational resources and energy required to process language tasks, making them more environmentally friendly and cost-effective.\n",
            "5. **Improved Accuracy**: Faster language models can process more data and perform more iterations, leading to improved accuracy and better performance in various NLP tasks.\n",
            "6. **Enhanced User Experience**: Fast language models can enable more interactive and engaging user experiences, such as conversational interfaces, voice-controlled devices, and augmented reality applications.\n",
            "7. **Competitive Advantage**: In today's fast-paced digital landscape, companies that can deploy fast and accurate language models can gain a competitive advantage over their rivals, improving customer satisfaction and driving business success.\n",
            "8. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to explore new ideas, test hypotheses, and iterate on models more quickly.\n",
            "9. **Edge Computing**: Fast language models are essential for edge computing applications, where data is processed in real-time on devices such as smartphones, smart home devices, or autonomous vehicles.\n",
            "10. **Specialized Domains**: Fast language models can be fine-tuned for specialized domains such as healthcare, finance, or law, where timely and accurate language processing is critical.\n",
            "\n",
            "To achieve fast language models, researchers and developers employ various techniques, including:\n",
            "\n",
            "1. Model pruning and knowledge distillation\n",
            "2. Quantization and binarization\n",
            "3. Efficient neural network architectures\n",
            "4. Parallel processing and distributed computing\n",
            "5. GPU acceleration and specialized hardware\n",
            "6. Caching and memoization\n",
            "7. Approximation algorithms and sampling techniques\n",
            "\n",
            "By leveraging these techniques, fast language models can unlock new possibilities in NLP, enabling applications that were previously unimaginable.\n"
          ]
        }
      ],
      "source": [
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}\n",
        "    ],\n",
        "    model=\"llama3-70b-8192\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqBnIixVYWKm"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, client: Groq, system: str = \"\") -> None:\n",
        "        self.client = client\n",
        "        self.system = system\n",
        "        self.messages: list = []\n",
        "        if self.system:\n",
        "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
        "\n",
        "    def __call__(self, message=\"\"):\n",
        "        if message:\n",
        "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "        result = self.execute()\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
        "        return result\n",
        "\n",
        "    def execute(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"llama3-70b-8192\", messages=self.messages\n",
        "        )\n",
        "        return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ln8H8qX7YWKn"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 4 * 7 / 3\n",
        "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "\n",
        "get_planet_mass:\n",
        "e.g. get_planet_mass: Earth\n",
        "returns weight of the planet in kg\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: What is the mass of Earth times 2?\n",
        "Thought: I need to find the mass of Earth\n",
        "Action: get_planet_mass: Earth\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 5.972e24\n",
        "\n",
        "Thought: I need to multiply this by 2\n",
        "Action: calculate: 5.972e24 * 2\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 1,1944×10e25\n",
        "\n",
        "If you have the answer, output it as the Answer.\n",
        "\n",
        "Answer: The mass of Earth times 2 is 1,1944×10e25.\n",
        "\n",
        "Now it's your turn:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def calculate(operation: str) -> float:\n",
        "    return eval(operation)\n",
        "\n",
        "\n",
        "def get_planet_mass(planet) -> float:\n",
        "    match planet.lower():\n",
        "        case \"earth\":\n",
        "            return 5.972e24\n",
        "        case \"jupiter\":\n",
        "            return 1.898e27\n",
        "        case \"mars\":\n",
        "            return 6.39e23\n",
        "        case \"mercury\":\n",
        "            return 3.285e23\n",
        "        case \"neptune\":\n",
        "            return 1.024e26\n",
        "        case \"saturn\":\n",
        "            return 5.683e26\n",
        "        case \"uranus\":\n",
        "            return 8.681e25\n",
        "        case \"venus\":\n",
        "            return 4.867e24\n",
        "        case _:\n",
        "            return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vPVZbRRIYWKo"
      },
      "outputs": [],
      "source": [
        "neil_tyson = Agent(client=client, system=system_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luMVECH3YWKp",
        "outputId": "c54fb6be-1c35-4e84-fffa-e71e75420dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Thought: I need to find the mass of Earth and Saturn, and then multiply the sum by 2.\n",
            "2\n",
            "Thought: I need to find the mass of Earth.\n",
            "3\n",
            "Action: get_planet_mass: Earth\n",
            "PAUSE\n",
            "Inside loop  [('get_planet_mass', 'Earth')]\n",
            "Observation: 5.972e+24\n",
            "4\n",
            "Thought: Now I need to find the mass of Saturn.\n",
            "5\n",
            "Action: get_planet_mass: Saturn\n",
            "PAUSE\n",
            "Inside loop  [('get_planet_mass', 'Saturn')]\n",
            "Observation: 5.683e+26\n",
            "6\n",
            "Thought: I have the mass of Earth and Saturn, now I need to add them together.\n",
            "7\n",
            "Action: calculate: 5.972e24 + 5.683e26\n",
            "PAUSE\n",
            "Inside loop  [('calculate', '5.972e24 + 5.683e26')]\n",
            "Observation: 5.74272e+26\n",
            "8\n",
            "Thought: Now I need to multiply the sum by 2.\n",
            "9\n",
            "Action: calculate: 5.74272e26 * 2\n",
            "PAUSE\n",
            "Inside loop  [('calculate', '5.74272e26 * 2')]\n",
            "Observation: 1.148544e+27\n",
            "10\n",
            "Answer: The mass of Earth plus the mass of Saturn and all of that times 2 is 1.148544e+27.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def agent_loop(max_iterations=10, query: str = \"\"):\n",
        "\n",
        "    agent = Agent(client=client, system=system_prompt)\n",
        "\n",
        "    tools = [\"calculate\", \"get_planet_mass\"]\n",
        "\n",
        "    next_prompt = query\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < max_iterations:\n",
        "        i += 1\n",
        "        print(i)\n",
        "        result = agent(next_prompt)\n",
        "        print(result)\n",
        "\n",
        "        if \"PAUSE\" in result and \"Action\" in result:\n",
        "            action = re.findall(r\"Action: ([a-z_]+): (.+)\", result, re.IGNORECASE)\n",
        "            print(\"Inside loop \" , action)\n",
        "            chosen_tool = action[0][0]\n",
        "            arg = action[0][1]\n",
        "\n",
        "            if chosen_tool in tools:\n",
        "                result_tool = eval(f\"{chosen_tool}('{arg}')\")\n",
        "                next_prompt = f\"Observation: {result_tool}\"\n",
        "\n",
        "            else:\n",
        "                next_prompt = \"Observation: Tool not found\"\n",
        "\n",
        "            print(next_prompt)\n",
        "            continue\n",
        "\n",
        "        if \"Answer\" in result:\n",
        "            break\n",
        "\n",
        "\n",
        "agent_loop(query=\"What is the mass of Earth plus the mass of Saturn and all of that times 2?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIsZXRfQYWKq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}